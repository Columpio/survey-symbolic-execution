% !TEX root = main.tex

\begin{figure}[t]
\centering
\includegraphics[width=0.35\columnwidth]{images/concrete-abstract.eps} 
\caption{Concrete and abstract excution machine models.}
\label{fig:concrete-symbolic}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Symbolic Executors}

\subsection{Concrete, Concolic, and Symbolic Execution}
\label{ss:concrete-concolic-symbolic}

[...]

\subsection{Principles of Symbolic Executors}
\label{ss:principles}

A symbolic execution engine should guarantee three main principles (\cite{MAYHEM-SP12}):
\begin{enumerate}
  \item the system should be able to forward progress for arbitrarily long time, ideally forever, without exceeding the given resources
  \item to maximize performance, no work should be repeated (avoid to restart symbolic / concrete execution)
  \item the system should reuse as much as possible previous analysis results
\end{enumerate}

Based on these principles, symbolic executors can be divided into:

\begin{itemize}
  \item {\em offline} executors (e.g., \cite{SAGE-NDSS08}): one path at time, every run independent from the others, results can be immediately reused, each run restarts the execution of the program from the beginning. In order to perform a run, two inputs must be provided: the target program and a seed input. The program is concretely executed and a trace is recorded. Then the trace is symbolically executed. This can be seen as a form of {\em concolic} execution (see Section~\ref{ss:concrete-concolic-symbolic}).
  \item {\em online} executors (e.g., \cite{KLEE-OSDI08,CKC-TOCS12,AEG-NDSS11}: for each fork, the execution state is cloned. All active execution states are kept in memory, no need to re-execute but huge burden on memory resources. A form of {\em context switch} is often needed. Executors may stop forking at a certain point to allow progress, but then some path are ignored. Memory is saved by aggressive copy-on-write optimization (e.g., immutable state). DFS can be used as exploration strategy in order to minimize memory consumption but can be very slow at doing progress. Notice that since multiple runs may be executed in parallel, isolation must be guaranteed (e.g., keeping different states of the OS by emulating system calls).
  \item {\em hybrid} executors (e.g., \cite{MAYHEM-SP12}): mixed approach. Start with an online approach, if needed switch to offline mode by doing checkpoints. A checkpoint contains the symbolic execution state and replay information. Concrete execution state is discarded since it can be quickly recovered at runtime by using one input generated by the solver before checkpointing.
\end{itemize}

\subsection{Consistency in Concolic Executors} \mynote{Provisionally placed here}

When exploring multiple paths at a time, an execution engine may execute some portions of code concretely, interleaving them with fully symbolic phases. This process must be done carefully in order to preserve the meaningfulness of the whole exploration.

When an argument $x$ for a function call to concretize is symbolic, the engine should convert it to some concrete value in order to perform the call. Compared to the family of possible paths from executing the call symbolically, this is equivalent to corset the exploration to a single path in the callee. When the call returns and the symbolic phase resumes, the concrete value for $x$ has then to become part of the path constraints for the remainder of the exploration: this may however result in a large number of paths being excluded.

\cite{CKC-TOCS12} presents the first systematic approach to consistently cross the symbolic/concrete boundary in both directions. In particular, the work presents a strategy to deal with constraints introduced on symbolic values as a consequence of concretization, and introduces a number of consistency models - where a state is {\em consistent} when there exists a feasible path to it from the initial state - which suit different analyses.

When execution returns to the symbolic domain, we have observed that updating constraints to reflect a concrete assignment might result in a corseting of the family of future paths that can be explored. Such constraints are marked in ~\cite{CKC-TOCS12} as {\em soft}, and whenever a branch in the symbolic domain is disabled because of a soft constraint, execution goes back and picks a value for the concrete call that would enable that branch.

\subsection{Optimization strategies} \mynote{Provisionally placed here}
\label{function-summaries}

\subsubsection{Function caching} The same function $f$, but more in general any part of a program, may be called multiple times during the execution of a program. These invocations may occur always at the same calling context or at different calling contexts. The traditional symbolic execution approach requires to symbolic execute the function $f$ every time it is called. \cite{G-POPL07} proposes a compositional approach that dynamically generates {\em function summaries}, allowing the symbolic execution engine to effectively reuse prior discovered analysis' results. A similar idea has been also proposed by~\cite{BCE-TACAS08}. Their main intuition is that if two program states differ only for some program values that are not subsequently read, the executions generate by these two program states will produce the same subsequent side effects. For this reason, side effects of a portion of code can be cached and possibly later reused. Since the two techniques are almost equivalent, we further discuss function summaries as introduced by~\cite{G-POPL07}.

\paragraph{Definition of function summaries} A function summary $\phi_f$ for a function $f$ is defined as a formula of propositional logic. It can be computed by successive iterations and defined as a disjunction of formulas $\phi_w$ of the form $\phi_w = {pre}_w \wedge post_w$, where $w$ is a possible execution path of function $f$, $pre_w$ is a conjunction of constraints over the inputs of $f$, and $post_w$ is a conjunction of constrains over the outputs of $f$. Formally, $\phi_f = \bigvee \phi_w$.  

\paragraph{Using function summaries} Whenever a function $f$ is called, the symbolic execution engine checks whether a summary $\phi_w$ of $f$ with $pre_w$ compliant with the current path constraints is available. If it is available, the post conditions $post_w$ are added to the current symbolic state. On the other hand, if no matching summary is found, a new function summary is computed.

\paragraph{Computing function summaries} Function summaries can be dynamically computed: whenever there is an invocation of a function $f$: $pre_w$ is obtained by the current set of constraints over the input of $f$, while $post_w$ is given by tracking constraints over the concolic execution of function $f$ over some concrete inputs that are compliant with $pre_w$. Notice that $pre_w$ defines an equivalence class of concrete executions that result in executions characterized by $post_w$. 

\paragraph{Issues} If the symbolic execution engine cannot reason on one or more statements contained in a function $f$, then the generated summary cannot be blindly reused. For instance, consider a function that contains a call to an external function (e.g., a syscall) or a {\em complex} function (e.g., a robust hash function). In this case, even if a matching function summary is found, the related post conditions $post_w$ may not be valid since they have been generated over a concrete execution and thus they cannot be generalized.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myinput{memory}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myinput{complex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Loops}
\label{se:loops}

Loops are one of the main cause for path explosion in symbolic execution. Indeed, each iteration of a loop can be seen as a {\tt IF-GOTO} statement, leading to a conditional branch in the execution tree. If the loop condition involves one or more symbolic values, the number of generated branches may even be infinite. For instance, consider the following example (taken from~\cite{CS-CACM13}):
    \begin{lstlisting}[basicstyle=\ttfamily\small]
    1.  int N = sym_input(); // e.g., read
    2.  while (N > 0) {
    3.    N = sym_input();  
    4.  }
    \end{lstlisting}
The path constraint set of any final state will contain:
  \[ \left ( \bigwedge_{i \in [1, n]]} N_i > 0 \right ) \wedge (N_{n+1} \leq 0) \]
where $N_i$ is the symbol introduced at the $i$-th iteration.\\

The problem of path explosion due to symbolic execution of loops has been attacked from different sides. A first natural strategy adopted by many symbolic engine is to limit the loop exploration up to a certain number of iterations. Obviously, this may lead to missing interesting paths in the program. For this reason, some prior works, such as~\cite{AEG-NDSS11}, had also considered the opposite strategy, allowing the engine to fully explore some loops. Indeed, this has been shown to be effective in some application contexts such as security (e.g., identidication fo buffer overflows) where interesting behavior may happen at the boundaries.

Using static or dynamic analysis techniques, it may be possible to derive some properties over a loop of a program. These can be exploited by an engine to significantly prune branching paths. For instance, knowing the exact number of iterations or, at least a constant upper bound, can significantly help the engine. Section~\ref{precontioned-symbolic-execution} provides a more general discussion of how preconditions can help symbolic execution.

Many prior works have instead proposed to replace in the symbolic execution tree the possibly infinite states generated by a loop with a {\em summary} of its side-effects. An approximation of the side-effects of a loop can be obtained by computing a {\em fixpoint} (see, e.g.,~\cite{KKM-USEC05,BNS-SP06,CFB-ACSAC06})). If a program contains an assertion (i.e., a property that should hold) after the loop, this can be exploited by a symbolic engine for automatically discovering some invariants over the loop. In~\cite{PV-SPIN04}, this is achieved by iteratively using invariant straightening and approximation techniques. 

\cite{GL-ISSTA11} presents a technique that automatically derives partial summarizations for loops. A loop summarization is formalized similarly to a function summary (see Section~\ref{function-summaries}), using a set of preconditions $pre_{loop}$ and a set of postconditions $post_{loop}$. These are computed dynamically during the symbolic execution by reasoning on the dependency among loop conditions and symbolic variables. As soon as a loop summary is computed, it is cached for possibly subsequent reuse. This not only allows the symbolic engine to avoid redundant executions of the same loop under the same program state, but also make it possible to generalize the loop summary to cover even different executions of the same loop that run under different conditions. A main limitation of this approach is that it can generate summaries only for loops that interactively manipulate symbolic variable by a constant non-zero amount.

On the other hand, \cite{SST-ATVA13} has introduced a technique that analyzes cyclic paths in the control flow graph of a given program and produces {\em templates} that declarative describe the program states generated by these portions of code into a symbolic execution tree. By exploiting templates, the symbolic execution engine needs to explore a significant reduced number of program states. A drawback of this approach is that templates introduce quantifiers into the path constraints. In turn, this may significantly increase the burden on the constraint solver.

It has been also observed that loop executions may strictly depend on input features. {\em Loop-extended symbolic execution} (LESE)~\cite{SPM-ISSTA09} is able to effectively explore a loop whenever a grammar describing the input program is available. By relating {\em trip counts} (i.e., number of iterations for loops) with features of the program input, the program states generated by a loop can be explored in a very effective manner.

\begin{comment}
Many prior works have targeted the problem of mitigating the path explosion effect due to symbolic execution of loops. We briefly discuss the main ideas introduced by these papers. 
\begin{itemize}

  \item {\em preconditions}: the symbolic execution of a loop can be made easier if some preconditions are known on the symbolic variables involved in the loop. For instance, if the number of loop iterations is known, the engine can drastically prune branching paths. For instance, this information may be determined using some static analysis techniques. Section~\ref{precontioned-symbolic-execution} provides a more general discussion of how preconditions can help symbolic execution.

  \item {\em fixed vs fully exploration}: depending on the goal, a symbolic engine may decide to fully explore a loop (e.g., see heuristics presented in~\cite{AEG-NDSS11} and discussed in Section~\ref{heuristics}) or to explore only a fixed number of iterations (e.g., up to 3 iterations) in order to avoid path explosion.

  \item {\em approximations}: effects of a loop are often approximated using {\em fixpoints} (e.g., in~\cite{KKM-USEC05,BNS-SP06,CFB-ACSAC06}). A fixpoint F is an approximation of the effect of loop body on an execution state. F approximates the state after the execution of loop whenever the initial state before the loop was F (?). Transforming an execution state to a fixpoint state is defined as widening. Construction of the fixpoint:
  \begin{itemize}
    \item S1: state after first iteration
    \item S2: state after second iteration
    \item compare S1 and S2: assign bottom to each symbol that has been altered
    \item repeat until there is no difference between Si and Si+1
    \item if there is a branch inside the loop, then either the branch is known or its condition is on a symbol which has been assign to bottom. In this case, two parallel states are created and then compared.
  \end{itemize}

  \item {\em loop invariant symbolic execution (LISE)}: Loop invariants can be discovered automatically using iterative techniques such as explained in~\cite{PV-SPIN04}, through the use of invariant straightening and approximation. The main idea is to work backward from a property that should be checked and then systematically applies approximation to reach termination. This approach has been later extended for parallel programs in~\cite{SZ-VMCAI12}.

  \item {\em loop summarization}: \cite{GL-ISSTA11} presents a technique that automatically derives partial summarizations for loop executions. A loop summarization is formalized similarly to a function summary (see Section~\ref{function-summaries}), using a set of preconditions $pre_{loop}$ and a set of postconditions $post_{loop}$. These are computed dynamically during the symbolic execution by reasoning on the dependency among loop conditions and symbolic variables. As soon as a loop summary is computed, it is cached for possibly subsequent reuse. This not only allows the symbolic engine to avoid redundant executions of the same loop under the same program state, but also make it possible to generalize the loop summary to cover even different executions of the same loop that run under different conditions. A main limitation of this approach is that it can generate summaries only for loops that interactively manipulate symbolic variable by a constant non-zero amount.

  \item {\em loop-extended symbolic execution (LESE)}: \cite{SPM-ISSTA09} has introduced a novel technique called LESE that symbolically tracks {\em trip counts} (i.e., number of times each loop is executed) and relate this information to features of the program input. A practical drawback of this technique is that a specification of the input grammar must be provided by the user to the symbolic execution engine.

  \item {\em compact symbolic execution}: \cite{SST-ATVA13} has introduced a technique that analyzes cyclic paths in the control flow graph of a given program and generates {\tt templates} that declarative describe the program states generated by these portions of code into a symbolic execution tree. By exploiting these templates, the symbolic execution engine needs to explore a significant reduced number of program states. A drawback of this approach is that templates introduce quantifiers into the path constraints. In turn, this can significantly increase the burden on the constraint solver.

  \item \cite{ST-ISSTA12} and~\cite{OT-ATVA11} present two technique for driving the symbolic execution of program toward a given target, even in presence of cyclic paths such as loops.\mynote{[E] non so se vale la pena discutere i dettagli}
\end{itemize}

Notice that detection/analysis of loops can be done using techniques such as~\cite{SGL-TOPLAS96} (e.g., in~\cite{CFB-ACSAC06}).
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Subroutines and recursion}
\label{se:recursion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myinput{environment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myinput{explosion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myinput{constraints}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\myinput{binary}


